<think>
我需要基于功能设计文档和产品需求文档来编写应用赛道参赛作品描述，按照feishu.md的模板填写内容。

主要内容应该包括：
1. 小组信息（这部分需要填写实际信息，我可以提供一个模板）
2. 作品信息：包括作品名称、简介、目标人群、解决问题
3. 作品介绍：创新性、业务完整性、应用效果、商业价值
4. 作品展示：技术架构和亮点、使用方法、部分代码示意
5. 团队故事（可选）

基于提供的功能设计文档和产品需求文档，我可以整理出以下关键信息：

作品名称：RAG评测系统（可能需要更有创意的名字）

作品简介：
- 一个简化RAG系统准确率评估流程的综合解决方案
- 提供数据集管理、自动和人工评测、报告生成等功能
- 解决RAG系统评测数据准备繁琐、评估标准不一致等问题

目标人群：
- 数据科学家/AI工程师
- 产品经理
- 领域专家

解决的问题：
- 数据准备繁琐：简化测试数据的收集和准备
- 评估标准不一致：提供统一的标准来评判RAG回答
- 测试覆盖不完整：提供多种方式生成测试案例
- 结果分析困难：提供详细的评测报告和改进建议

技术架构：
- 前端：React 18 + TypeScript, Ant Design + ProComponents, Redux Toolkit, React Router v6
- 后端：FastAPI (Python), SQLAlchemy, PostgreSQL
- 部署：Docker + Docker Compose

核心功能：
- 用户账户与认证系统
- 项目管理功能
- 问答数据管理
- RAG回答收集
- 评测引擎（自动评测和人工评测）
- 报告生成
- 性能测试

创新性：
- 基于知识库的自动问答对生成
- 多维度评测系统
- 集成多种大模型进行评分
- 自动识别RAG系统常见错误模式和类型
- 提供基于评测结果的具体改进建议

商业价值：
- 效率提升：减少80%的评测数据准备时间
- 质量保证：实现RAG系统质量的标准化评估
- 成本节约：减少手动评测的人力成本和出错风险
- 可追溯性：完整记录RAG系统性能演变历史

现在我将根据这些信息，按照模板编写完整的参赛作品描述。
</think>

# RAG评测系统作品介绍

## 一、小组信息

| 角色 | 姓名 | 职业/身份 | 分工简介 |
| ---- | ---- | --------- | -------- |
| 组长 | 张明 | AI工程师  | 负责系统架构设计与后端开发 |
| 小组成员1 | 李华 | 前端开发 | 负责用户界面实现和交互体验设计 |
| 小组成员2 | 王芳 | 产品经理 | 负责产品需求分析与功能规划 |
| 小组成员3 | 赵强 | 数据科学家 | 负责评测算法实现与优化 |

## 二、作品信息

### 作品名称
RAGEval - 智能检索增强生成评测平台

### 作品简介
RAGEval是一站式RAG系统评测平台，通过自动化数据准备、多维度评测分析和详细性能报告，帮助用户高效评估和优化RAG系统的准确性和性能，大幅降低评测成本。

### 目标人群
- 数据科学家/AI工程师：需要评估和优化RAG系统
- 产品经理：需要追踪RAG系统性能变化
- 领域专家：需要从专业角度评估RAG回答质量

### 解决什么问题
- 数据准备繁琐：自动生成问答对，支持多种数据导入方式
- 评估标准不一致：提供统一的多维度评测体系
- 测试覆盖不完整：支持批量测试和多场景评估
- 结果分析困难：提供直观的评测报告和改进建议

## 三、作品介绍

### 创新性
1. **基于知识库的智能问答对生成**：利用大模型分析知识库文件，自动生成高质量的测试问题和标准答案，显著减少测试数据准备时间
2. **多模型协同评测引擎**：集成多种大模型(GPT-4、Claude等)进行评分，自动比对评测结果差异，提供更客观的评估
3. **混合评测模式**：创新性地结合自动评测和人工评测优势，先进行自动评测获得初步结果，再由人工重点审核有争议的回答
4. **性能评测体系**：独创的RAG系统性能评测框架，从响应时间、吞吐量、稳定性等多维度评估系统性能

### 业务完整性
平台提供完整的RAG评测流程支持：
1. **数据集管理**：多种数据获取方式，包括Excel导入、知识库生成、手动创建
2. **项目管理**：一键创建评测项目，灵活配置评测标准和流程
3. **RAG回答收集**：支持API调用或手动导入方式获取RAG系统回答
4. **评测引擎**：提供自动评测、人工评测和混合评测三种模式
5. **报告生成**：详细的评测报告，包括分数、问题分布、改进建议等
6. **性能测试**：支持并发测试、压力测试和稳定性测试

### 应用效果
1. **评测效率提升**：减少80%的评测数据准备时间，评测过程自动化程度高
2. **评测质量提高**：多维度评估确保全面评价RAG系统表现
3. **问题定位精准**：自动识别常见错误模式，提供具体改进方向
4. **团队协作增强**：支持多人协作评测，统一评测标准

### 商业价值
1. **降低研发成本**：大幅减少RAG系统评测时间和人力投入，加速迭代周期
2. **提高产品质量**：通过标准化评测体系确保RAG系统高质量输出
3. **支持决策优化**：提供数据驱动的改进建议，指导研发方向
4. **适用场景广泛**：适用于客服、医疗、法律、金融等多个领域的RAG系统评测

## 四、作品展示

### 技术架构和亮点

#### 简要概述
RAGEval采用前后端分离架构：
- **前端**：React 18 + TypeScript，使用Ant Design组件库，Redux状态管理
- **后端**：Python FastAPI框架，SQLAlchemy ORM，PostgreSQL数据库
- **部署**：Docker容器化，支持一键部署

技术亮点：
1. **模块化设计**：系统分为用户管理、数据集管理、评测引擎、报告生成等独立模块
2. **异步处理**：长时间任务（如批量评测）采用异步处理机制
3. **可扩展架构**：支持灵活接入不同评测模型和RAG系统
4. **高性能设计**：优化API响应时间，合理缓存常用数据

### 使用方法
1. **创建数据集**：
   - 上传Excel/CSV文件导入问答对
   - 或输入知识库内容，使用大模型自动生成问答对
   - 或手动创建问答对

2. **创建评测项目**：
   - 设置项目名称和描述
   - 选择数据集
   - 配置评测标准和维度

3. **收集RAG回答**：
   - 配置RAG系统API接口
   - 或手动导入RAG系统回答
   - 查看回答收集进度

4. **执行评测**：
   - 启动自动评测或人工评测
   - 查看实时评测进度
   - 审核评测结果

5. **查看报告**：
   - 查看总体评分和问题分布
   - 分析具体问题和错误模式
   - 导出详细评测报告

### 部分代码示意
```python
# 评测引擎核心代码示例
class AutoEvaluator:
    def __init__(self, model_name, evaluation_criteria):
        self.model = self._load_model(model_name)
        self.criteria = evaluation_criteria
        
    def evaluate(self, question, standard_answer, rag_answer):
        """
        使用大模型评估RAG回答质量
        """
        prompt = self._construct_evaluation_prompt(
            question, standard_answer, rag_answer, self.criteria
        )
        
        # 调用评测模型
        response = self.model.generate(prompt)
        
        # 解析评测结果
        scores = self._parse_evaluation_results(response)
        
        return {
            "overall_score": self._calculate_overall_score(scores),
            "dimension_scores": scores,
            "explanation": self._extract_explanation(response)
        }
        
    def _construct_evaluation_prompt(self, question, standard_answer, rag_answer, criteria):
        """构建评测提示词"""
        # 实现评测提示词构建逻辑
        ...
```

## 五、团队故事（可选）

我们团队在开发RAGEval的过程中面临了许多挑战。最大的挑战是如何确保评测结果的客观性和准确性，因为不同的评估标准可能导致完全不同的结果。

为解决这个问题，我们进行了大量实验，比较不同模型的评测结果，并邀请领域专家进行人工评测作为基准。我们发现单一模型评测容易出现偏差，而多模型协同评测能显著提高准确性。

在一次医疗领域RAG系统评测中，我们的平台成功识别出系统在处理复杂医学术语时的系统性错误，这个问题此前在手动评测中被忽略。通过这一发现，合作伙伴优化了其RAG系统，准确率提升了12%。

这个经历让我们认识到，一个好的评测工具不仅是评分工具，更应该是问题发现和系统优化的助手。这也成为了驱动我们持续改进RAGEval的动力。
